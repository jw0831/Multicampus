{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__파일 경로__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='파일 위치'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__사용자 정의 stopwords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.update(['say','get','know','crosstalk','people','want','joe','would','think','go','much','that']) # for news\n",
    "\n",
    "stopWords.update(['well','okay','yeah','right','thing','president','get','know','say']) # for debate\n",
    "\n",
    "stopWords.update [\"https\",\"http\",\"com\",\"donald\",\"trump\",\"joe\",\"biden\",\"would\", \"election\", \"could\", \"also\", \"president\",\"vote\", \"voters\", \"house\", \"run\", \"two\", \"rox\", \"reuters\",\"way\",\"go\", \"point\", \"take\", \"say\"] # for twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__품사에 따른 표제어 추출 함수 재정의__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WL = WordNetLemmatizer()\n",
    "\n",
    "def proper_lemmatize(word,tag):\n",
    "    \n",
    "    if tag[0]=='V':\n",
    "        ret= WL.lemmatize(word,'v')\n",
    "    elif tag[0]=='J':\n",
    "        ret= WL.lemmatize(word,'a')\n",
    "    elif tag[0]=='NNP':\n",
    "        ret= word\n",
    "    else:\n",
    "        ret= WL.lemmatize(word)\n",
    "        \n",
    "    return ret if ret not in stopWords else ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토론 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_debate = pd.read_csv(path+'us_election_2020_1st_presidential_debate.csv')\n",
    "second_debate = pd.read_csv(path+'us_election_2020_2nd_presidential_debate.csv')\n",
    "trump_town_hall = pd.read_csv(path+'us_election_2020_trump_town_hall.csv')\n",
    "biden_town_hall = pd.read_csv(path+'us_election_2020_biden_town_hall.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 , 2차 대선 토론 및 연설 데이터 후보에 따라 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_Biden = first_debate[first_debate.speaker=='Vice President Joe Biden']\n",
    "first_Trump = first_debate[first_debate.speaker=='President Donald J. Trump']\n",
    "\n",
    "second_Biden = second_debate[second_debate.speaker=='Joe Biden']\n",
    "second_Trump = second_debate[second_debate.speaker=='Donald Trump']\n",
    "\n",
    "town_hall_Trump = trump_town_hall[trump_town_hall.speaker=='President Trump']\n",
    "town_hall_Biden = biden_town_hall[biden_town_hall.speaker=='Joe Biden']\n",
    "\n",
    "Trump_speaking = first_Trump.text.values.tolist()+second_Trump.text.values.tolist()+town_hall_Trump.text.values.tolist()\n",
    "Biden_speaking = first_Biden.text.values.tolist()+second_Biden.text.values.tolist()+town_hall_Biden.text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump_speaking_token = []\n",
    "\n",
    "for sentence in Trump_speaking:\n",
    "    clean_words=[]\n",
    "    sentence = re.sub('[^a-zA-Z-]+',' ',sentence)\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        if len(word)>2 and word.lower() not in stopWords:\n",
    "            if word not in ['Trump','Biden']:\n",
    "                word = word.lower()\n",
    "            clean_words.append(word)\n",
    "    \n",
    "    Trump_speaking_token.append(clean_words)\n",
    "    \n",
    "for i in range(len(Trump_speaking_token)):\n",
    "    Trump_speaking_token[i] = [proper_lemmatize(word,tag) for (word,tag) in pos_tag(Trump_speaking_token[i])]\n",
    "    \n",
    "Trump_corpus = [*map(lambda x: ' '.join(x) , Trump_speaking_token)]\n",
    "\n",
    "Biden_speaking_token = []\n",
    "\n",
    "for sentence in Biden_speaking:\n",
    "    clean_words=[]\n",
    "    sentence = re.sub('[^a-zA-Z-]+',' ',sentence)\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        if len(word)>2 and word.lower() not in stopWords:\n",
    "            if word not in ['Trump','Biden']:\n",
    "                word = word.lower()\n",
    "            clean_words.append(word)\n",
    "    \n",
    "    Biden_speaking_token.append(clean_words)\n",
    "    \n",
    "for i in range(len(Biden_speaking_token)):\n",
    "    Biden_speaking_token[i] = [proper_lemmatize(word,tag) for (word,tag) in pos_tag(Biden_speaking_token[i])]\n",
    "    \n",
    "Biden_corpus = [*map(lambda x: ' '.join(x) , Biden_speaking_token)]\n",
    "\n",
    "Trump_corpus.to_csv('경로 포함 파일명')\n",
    "Biden_corpus.to_csv('경로 포함 파일명')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(path+\"news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기간 필터링 및 인덱스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(news[(news[\"Date\"] == \"2020-11-01\") | (news[\"Date\"] == \"2020-11-02\")].index)\n",
    "news = news.reset_index()\n",
    "news = news.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 프레임 상태로 전처리하기 위한 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_tokenize(element):\n",
    "\n",
    "    ret_list = []\n",
    "    \n",
    "    for sentence in element:\n",
    "        clean_words=[]\n",
    "        sentence = re.sub('[^a-zA-Z-]+',' ',sentence)\n",
    "\n",
    "        for word in word_tokenize(sentence):\n",
    "            if len(word)>2 and word.lower() not in stopWords:\n",
    "                if word not in ['Trump','Biden']:\n",
    "                    word = word.lower()\n",
    "                clean_words.append(word)\n",
    "\n",
    "        ret_list.append(clean_words)\n",
    "    return ret_list\n",
    "\n",
    "def proper_lemmatize(word,tag):\n",
    "    \n",
    "    if tag[0]=='V':\n",
    "        ret= WL.lemmatize(word,'v')\n",
    "    elif tag[0]=='J':\n",
    "        ret= WL.lemmatize(word,'a')\n",
    "    elif tag[0]=='NNP':\n",
    "        ret= word\n",
    "    else:\n",
    "        ret= WL.lemmatize(word)\n",
    "        \n",
    "    return ret if ret not in stopWords else ''\n",
    "\n",
    "def apply_lemmetize(element):\n",
    "    \n",
    "    for i in range(len(element)):\n",
    "        element[i] = [proper_lemmatize(word,tag) for (word,tag) in pos_tag(element[i])]\n",
    "        \n",
    "    return element        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"Text\"] = news[\"Text\"].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "news['Text'] = news['Text'].apply(proper_tokenize)\n",
    "news['Text'] = news['Text'].apply(proper_tokenize)\n",
    "\n",
    "news['Text'] = news['Text'].apply(apply_lemmetize)\n",
    "news['Text'] = news['Text'].apply(apply_lemmetize)\n",
    "\n",
    "news['Text'] = news['Text'].apply(lambda x : [*map(lambda y: ' '.join(y) , x)])\n",
    "news['Text'] = news['Text'].apply(lambda x : [*map(lambda y: ' '.join(y) , x)])\n",
    "\n",
    "new.to_csv('경로명 포함 파일명')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_biden=pd.read_csv(path+\"from_Biden_without_hashtag.csv\")\n",
    "to_biden=pd.read_csv(path+\"to_Biden_without_hashtag.csv\")\n",
    "from_trump=pd.read_csv(path+\"from_Trump_without_hashtag.csv\")\n",
    "to_trump=pd.read_csv(path+\"to_Trump_without_hashtag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 , 역토큰화 진행 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(document):\n",
    "    token = []\n",
    "    for sentence in document:\n",
    "        clean_words=[]\n",
    "        sentence = re.sub('[^a-zA-Z-]+',' ',sentence)\n",
    "\n",
    "        for word in word_tokenize(sentence):\n",
    "            if len(word)>2 and word.lower() not in stopWords:\n",
    "                if word not in ['Trump','Biden']:\n",
    "                    word = word.lower()\n",
    "                clean_words.append(word)\n",
    "        token.append(clean_words)\n",
    "    return token\n",
    "\n",
    "def make_sent_corpus(lemmatized_token):\n",
    "    sent_corpus = [*map(lambda x: ' '.join(x) , lemmatized_token)]\n",
    "    return sent_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_biden=from_biden.text.values.tolist()\n",
    "to_biden=to_biden.text.values.tolist()\n",
    "from_trump=from_trump.text.values.tolist()\n",
    "to_trump=to_trump.text.values.tolist()\n",
    "\n",
    "from_biden=tokenizing(from_biden)\n",
    "to_biden=tokenizing(to_biden)\n",
    "from_trump=tokenizing(from_trump)\n",
    "to_trump=tokenizing(to_trump)\n",
    "\n",
    "from_biden=lemmatizer(from_biden)\n",
    "to_biden=lemmatizer(to_biden)\n",
    "from_trump=lemmatizer(from_trump)\n",
    "to_trump=lemmatizer(to_trump)\n",
    "\n",
    "from_biden_corpus=make_sent_corpus(from_biden)\n",
    "to_biden_corpus=make_sent_corpus(to_biden)\n",
    "from_trump_corpus=make_sent_corpus(from_trump)\n",
    "to_trump_corpus=make_sent_corpus(to_trump)\n",
    "\n",
    "from_biden_corpus.to_csv('경로명 포함 파일명')\n",
    "to_biden_corpus.to_csv('경로명 포함 파일명')\n",
    "from_trump_corpus.to_csv('경로명 포함 파일명')\n",
    "to_trump_corpus.to_csv('경로명 포함 파일명')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
