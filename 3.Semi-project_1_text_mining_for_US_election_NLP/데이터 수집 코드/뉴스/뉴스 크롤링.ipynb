{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime ,timedelta , date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __크롬 드라이버 연동__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'D:/downloads/'\n",
    "driver = webdriver.Chrome(my_path+'chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fox 뉴스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load more 버튼을 cnt만큼 눌러서 모든 기사를 페이지에 로딩하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_news(driver,cnt):\n",
    "    while cnt:\n",
    "        try:\n",
    "            driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/div[2]/div/main/section/footer/div/a').click()\n",
    "            time.sleep(1.5)\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "            break\n",
    "        cnt-=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 기사 태깅 후에 `12 hours ago` 와 같은 정보로 부터 날짜를 계산하는 함수:  \n",
    "- 'age' 형식은 현재 시간에서 빼는 식으로  \n",
    "- 'October 24' 형식은 그대로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(tag): # 뉴스의 날짜 post date 계산\n",
    "    global current_time\n",
    "    \n",
    "    posted_time_str = tag.select('div.info > header > div > div > span.time')[0].text\n",
    "    \n",
    "    if 'ago' in posted_time_str :\n",
    "        \n",
    "        time_diff = re.search('\\d+',posted_time_str).group()\n",
    "        \n",
    "        if 'day' in posted_time_str:\n",
    "            posted_time = current_time - timedelta(days = int(time_diff))\n",
    "        elif 'hour' in posted_time_str:\n",
    "            posted_time = current_time - timedelta(hours = int(time_diff))\n",
    "        elif 'min' in posted_time_str:\n",
    "            posted_time = current_time - timedelta(minutes = int(time_diff))\n",
    "        else:\n",
    "            print('error from 1')\n",
    "\n",
    "    else:\n",
    "        \n",
    "        post_date = re.search('\\d+',posted_time_str).group()\n",
    "        \n",
    "        if 'tober' in posted_time_str:\n",
    "            posted_time = datetime(2020,10,int(post_date))\n",
    "        elif 'tember' in posted_time_str:\n",
    "            posted_time = datetime(2020,9,int(post_date))\n",
    "        else:\n",
    "            print('error from 2')\n",
    "    \n",
    "    return date.isoformat(posted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now()\n",
    "data=pd.DataFrame()\n",
    "load_more = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.foxnews.com/category/politics/2020-presidential-election')\n",
    "time.sleep(2)\n",
    "\n",
    "load_all_news(driver,load_more)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "articles = soup.select('#wrapper > div.page > div.page-content > div > main > section > div > article')\n",
    "\n",
    "news = [ {'headlines':article.select('div.info > header > h4 > a')[0].text,\n",
    "         'date':get_date(article),\n",
    "         'Press':'Fox',\n",
    "         'Link':'https://www.foxnews.com'+article.select('div.info > header > h4 > a')[0].attrs['href'],\n",
    "         'category':article.select('div.info > header > div > div > span.eyebrow > a')[0].text} for article in articles]\n",
    "news_df = pd.DataFrame(news)\n",
    "data = pd.concat([data,news_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수집된 링크들로부터 각각 본문 내용 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_text_list=[]\n",
    "for url in data['Link']:\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    body = soup.select('#wrapper > div.page-content > div.row.full > main > article > div > div.article-content > div.article-body')\n",
    "    body_text = ''.join([p.text for p in body[0].select('p')])\n",
    "    body_text_list.append(body_text)\n",
    "    \n",
    "data['text'] = pd.Series(body_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBS & POLITICO 뉴스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 로직 (CBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbs_news(page_num):\n",
    "    \n",
    "    content=[]\n",
    "    title=[]\n",
    "    date=[]\n",
    "    \n",
    "    driver = webdriver.Chrome(path+\"chromedriver.exe\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(1,page_num+1):\n",
    "        \n",
    "        driver.get(f'https://www.cbsnews.com/feature/election-2020/{i}/')\n",
    "        time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        soup=soup.select('div.col-8.nocontent')[0].select('section')\n",
    "        \n",
    "        try:\n",
    "            for j in range(1,2): #26\n",
    "\n",
    "                title.append(soup[0].select('h4.item__hed')[j-1].text.strip())\n",
    "                date.append(soup[0].select('span.item__date')[0].text)\n",
    "\n",
    "                driver.find_element_by_xpath(f'//*[@id=\"component-election-2020\"]/div/article[{j}]/a').click()\n",
    "                time.sleep(2)\n",
    "\n",
    "                html_content = driver.page_source\n",
    "                soup_content = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "                lis=soup_content.select('article')[0].select('p')\n",
    "\n",
    "                if lis[0].text.strip()[:2]=='By':\n",
    "                    con=''\n",
    "                    for k in lis[2:]:\n",
    "                        con+=k.text.strip() \n",
    "\n",
    "                else:\n",
    "                    con=''\n",
    "                    for k in lis:\n",
    "                        con+=k.text.strip()\n",
    "\n",
    "\n",
    "                content.append(con)\n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    df=pd.DataFrame({'date':date,'title':title,'content':content})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인코드 (CBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=cbs_news(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "data['date']=data['date'].str.replace('Oct','2020 10').replace('Sep','2020 09')\n",
    "data['date'][0:n+1]=datetime.date.today()\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인로직 (POLITICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politico_news(page_num):\n",
    "    \n",
    "    content=[]\n",
    "    title=[]\n",
    "    date=[]\n",
    "\n",
    "    driver = webdriver.Chrome(path+\"chromedriver.exe\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    for i in range(1,page_num+1):\n",
    "\n",
    "        driver.get(f'https://www.politico.com/news/2020-elections/{i}')\n",
    "        time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            for j in range(1,20): #20\n",
    "\n",
    "                driver.find_element_by_xpath(f'//*[@id=\"main\"]/div[3]/div/div/section[1]/div[2]/ol/li[{j}]/article/div/div/div/div/header/h1/a').click()\n",
    "                time.sleep(2)\n",
    "\n",
    "                html_content = driver.page_source\n",
    "                soup_content = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "                lis=soup_content.select('article')[0].select('p')\n",
    "\n",
    "                con=''\n",
    "                for k in lis:\n",
    "                    con+=k.text.strip() \n",
    "\n",
    "                content.append(con)\n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "\n",
    "                title.append(soup.select('ol.front-list')[0].select('h1')[j-1].text.strip())\n",
    "                d=soup.select('ol.front-list')[0].select('p.timestamp')[j-1].text[:10]\n",
    "\n",
    "                if d=='31/08/2020':\n",
    "                    break\n",
    "\n",
    "                date.append(d)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df=pd.DataFrame({'date':date,'title':title,'content':content})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인코드 (POLOTICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=politico_news(227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRP & Reuters 뉴스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 코드 (NRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.npr.org/sections/elections/\") # get page\n",
    "\n",
    "n = 0\n",
    "while 1 :\n",
    "    driver.find_element_by_css_selector(\"#main-section > div.options.has-more-results > button\").click() # load 15 times\n",
    "    n += 1\n",
    "    if n > 14:\n",
    "        break\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "link_html = soup.select(\"div.item-info-wrap > div > h2 > a\")  # get links\n",
    "links = []\n",
    "for link in link_html:\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "    \n",
    "df = []               # crawl data from every link\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    tm.sleep(5)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    try : \n",
    "        title = soup.select(\"#main-section > article > div.storytitle > h1\")[0].text # 기사제목\n",
    "        date = soup.select(\"#story-meta > div.story-meta__one > div > time > span\")[0].text # 기사 날짜\n",
    "        time = soup.select(\"#story-meta > div.story-meta__one > div > time > span\")[1].text #기사 시간\n",
    "        textall = soup.select(\"#storytext > p\") # 기사 내용\n",
    "\n",
    "        text = []\n",
    "        for t in textall :\n",
    "            text.append(t.text)\n",
    "            \n",
    "        topic = soup.select(\"#main-section > article > div.slug-wrap > h3 > a\")[0].text # 주제\n",
    "\n",
    "    except:\n",
    "        title = np.nan\n",
    "        date = np.nan\n",
    "        time = np.nan\n",
    "        text = np.nan\n",
    "\n",
    "    article = {\"Date\" : date, \"Time\": time, \"Press\" : \"NPR\", \"Topic\": topic, \"Title\": title, \"Text\" : text} # 합치기 \n",
    "    df.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.DataFrame(df)\n",
    "npr.dropna(inplace = True)\n",
    "npr.drop_duplicates([\"Title\"])\n",
    "npr[npr[\"Date\"].str.contains('August')].index\n",
    "npr = npr.loc[:340, :]\n",
    "npr.to_csv(\"nprdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인코드 (Reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.reuters.com/news/archive/us-elections-2020?view=page&page=1&pageSize=10\") # get page\n",
    "\n",
    "links = []\n",
    "for page in range(1,88):\n",
    "    url = f\"https://www.reuters.com/news/archive/us-elections-2020?view=page&page={page}&pageSize=10\" # get links\n",
    "    driver.get(url)\n",
    "    tm.sleep(4)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    link_html = soup.select(\"div > div.column1.col.col-10 > section > section > div > article > div.story-content > a\")\n",
    "    for link in link_html:\n",
    "        links.append(\"https://www.reuters.com/\" + link.get('href'))\n",
    "        \n",
    "        \n",
    "df = []                     # crawl data from every link\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    tm.sleep(5)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try :\n",
    "        # 기사제목 \n",
    "        title = soup.select(\"#__next > div > div.TwoColumnsLayout-hero-3H8pu > div.TwoColumnsLayout-content-2D7Bp.TwoColumnsLayout-left-column-CYquM.ArticlePage-hero-container-1BcoN > div > div > h1\")[0].text\n",
    "        # 기사 날짜\n",
    "        date = soup.select(\"#__next > div > div.TwoColumnsLayout-hero-3H8pu > div.TwoColumnsLayout-content-2D7Bp.TwoColumnsLayout-left-column-CYquM.ArticlePage-hero-container-1BcoN > div > div > div > div > time\")[0].text\n",
    "        #기사 시간\n",
    "        time = soup.select(\"#__next > div > div.TwoColumnsLayout-hero-3H8pu > div.TwoColumnsLayout-content-2D7Bp.TwoColumnsLayout-left-column-CYquM.ArticlePage-hero-container-1BcoN > div > div > div > div > time\")[1].text\n",
    "        # 주제\n",
    "        topic = soup.select(\"#__next > div > div.TwoColumnsLayout-hero-3H8pu > div.TwoColumnsLayout-content-2D7Bp.TwoColumnsLayout-left-column-CYquM.ArticlePage-hero-container-1BcoN > div > div > div > a\")[0].text\n",
    "        # 기사 내용\n",
    "        textall = soup.select(\"#__next > div > div.TwoColumnsLayout-body-86gsE.ArticlePage-body-container-10RhS > div.TwoColumnsLayout-left-column-CYquM > article\")\n",
    "        text = []\n",
    "        for t in textall :\n",
    "            text.append(t.text)\n",
    "    except:\n",
    "        title = np.nan\n",
    "        date = np.nan\n",
    "        time = np.nan\n",
    "        text = np.nan     \n",
    "        topic = np.nan\n",
    "\n",
    "    # 합치기 \n",
    "    article = {\"Date\" : date, \"Time\": time, \"Press\" : \"Reuters\", \"Topic\": topic, \"Title\": title, \"Text\" : text}\n",
    "    df.append(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = pd.DataFrame(df)\n",
    "rt.dropna(inplace = True)\n",
    "rt[rt[\"Date\"].str.contains('August')]\n",
    "rt = rt.loc[:868, :]\n",
    "rt[rt[\"Date\"].str.contains('November')]\n",
    "rt = rt.loc[36:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중도 데이터 통합 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = pd.concat([npr, rt])\n",
    "center.dropna(inplace = True)\n",
    "center.to_csv(\"center.201103.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
